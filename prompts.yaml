analysis:
  template: |
    Compare these two Python code snippets for semantic differences:
    
    Code 1:
    ```python
    {code1}
    ```
    
    Code 2:
    ```python
    {code2}
    ```
    
    {previous_analysis}
    {coverage_info}
    
    Analyze:
    1. What are the semantic differences (behavior, not just syntax)?
    2. What edge cases might expose differences?
    3. What input ranges should be tested?
    4. Are there boundary conditions, error cases, or special values to consider?
    
    Be specific and technical.
  version: "1.0.0"
  description: "Initial semantic analysis of two code snippets"

test_generation:
  template: |
    Based on this analysis:
    {analysis}

    Generate exactly {num_tests} test cases for a function with {num_params} parameter(s).

    Return ONLY valid JSON array, no markdown, no explanation:
    [{{"input": [5], "description": "normal"}}, {{"input": [0], "description": "zero"}}]
  version: "1.0.0"
  description: "Generate targeted test cases based on analysis"

coverage_evaluation:
  template: |
    Given this code comparison analysis:
    {analysis}
    
    And these test cases that were run:
    [{{"gap": "description of what's not tested", "why_important": "why this matters"}}]
    
    Differences found: {num_differences} differences
    
    Adversarially evaluate:
    1. What edge cases are NOT covered?
    2. What boundary conditions are missing?
    3. What error scenarios haven't been tested?
    4. Are there input combinations that might reveal hidden differences?
    
    Be critical and specific. Return a JSON array of coverage gaps:
    [{{"gap": "description of what's not tested", "why_important": "why this matters"}}]
  version: "1.0.0"
  description: "Adversarially evaluate test coverage and identify gaps"

semantic_diff:
  template: |
    You are analyzing two Python functions that have been tested extensively.
    
    Code 1:
    ```python
    {code1}
    ```
    
    Code 2:
    ```python
    {code2}
    ```
    
    **Test Results:**
    - {num_differences} behavioral differences were found during testing
    - {num_tests} test cases were executed
    - Coverage gaps remaining: {coverage_gaps}
    
    **Your Task:**
    Produce a diff that shows ONLY the lines that cause semantic/behavioral differences.
    Ignore refactoring, variable renames, or stylistic changes that don't affect behavior.
    
    Focus on:
    - Lines that handle edge cases differently (missing keys, None values, empty collections)
    - Lines with different error handling or validation logic
    - Lines that compute results differently
    
    **Output Format:**
    Use standard diff format, but ONLY include hunks for semantically different sections.
    Mark each hunk with a comment explaining the behavioral difference.
    
    Example output:
    ```diff
    @@ Line 12: Error handling @@
    - discounted_amount = txn['discount']  # Crashes if key missing
    + discounted_amount = txn.get('discount', 0)  # Defaults to 0 if missing
    
    @@ Line 25: Division by zero @@
    - avg = total / count  # Can crash
    + avg = total / count if count > 0 else 0  # Safe handling
    ```
    
    Provide ONLY the diff output with comments. No introduction or conclusion.
  version: "1.0.1"
  description: "Generate final semantic diff visualization"

# Model configuration
model_config:
  default_model: "gemini-2.5-flash"
  default_temperature: 0.7
  max_output_tokens: 1000000
